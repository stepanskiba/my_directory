{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_train.csv').astype('float64')\n",
    "y = np.array(pd.read_csv('Y_train.csv')).ravel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def data_clearing(X):\n",
    "    bad_columns = X.columns[X.isna().sum() > 10000]\n",
    "    X = X.drop(columns=bad_columns, axis=0)  # в некоторых колонках слишком много nan, удалим эти колонки\n",
    "    X = X.drop(X.columns[X.nunique() == 1], axis=1)  # удалим, где только 1 значение\n",
    "    return X\n",
    "\n",
    "\n",
    "def correlation(data):\n",
    "    \"\"\"Строим матрицу корреляций\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.heatmap(data.corr(), annot=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def delete_corr_columns(X, value_of_corr=0.9):\n",
    "    corr_matrix = X.corr().abs()  # получаем таблицу корреляции и модуля корреляции\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))  # создаем маску для верхнего треугольника таблицы\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[\n",
    "                                                                 column] >= value_of_corr)]  # находим колонки, которые имеют корреляцию больше или равную value_of_corr\n",
    "    X = X.drop(to_drop, axis=1)  # удаляем колонки из DataFrame\n",
    "    return X\n",
    "\n",
    "\n",
    "X = data_clearing(X)\n",
    "#correlation(X)\n",
    "X = delete_corr_columns(X)\n",
    "#correlation(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def standartization_data(X_train, X_valid):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    X_valid_norm = scaler.transform(X_valid)\n",
    "    return X_train_norm, X_valid_norm\n",
    "\n",
    "\n",
    "def min_max_scaler(X_train, X_valid):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_min_max = scaler.fit_transform(X_train)\n",
    "    X_test_min_max = scaler.transform(X_valid)\n",
    "    return X_train_min_max, X_test_min_max\n",
    "\n",
    "\n",
    "X_train_norm, X_valid_norm = standartization_data(X_train, X_valid)\n",
    "X_train_min_max, X_valid_min_max = min_max_scaler(X_train, X_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def metrics(y_valid, y_pred):\n",
    "    mse = mean_squared_error(y_valid, y_pred)\n",
    "    mae = mean_absolute_error(y_valid, y_pred)\n",
    "    r2 = r2_score(y_true=y_valid, y_pred=y_pred)\n",
    "\n",
    "    # Выводим значения метрик\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R2 score:\", r2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def plot_residuals(y_valid, y_pred):\n",
    "    residuals = y_valid - y_pred\n",
    "\n",
    "    # Строим график остатков\n",
    "    plt.scatter(y_pred, residuals)\n",
    "    plt.title(\"Residual plot\")\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_true_vs_pred(y_valid, y_pred):\n",
    "    plt.scatter(y_valid, y_pred)\n",
    "    plt.title(\"True vs predicted values\")\n",
    "    plt.xlabel(\"True values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], 'r--')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.003629757185708279\n",
      "MAE: 0.036961861958909825\n",
      "R2 score: 0.4146579576709387\n",
      "MSE: 0.003629757185708281\n",
      "MAE: 0.036961861958910006\n",
      "R2 score: 0.4146579576709384\n",
      "MSE: 0.003629757185708282\n",
      "MAE: 0.03696186195891003\n",
      "R2 score: 0.41465795767093816\n"
     ]
    }
   ],
   "source": [
    "#LinearRegression\n",
    "def Linear_Regression(X_train, X_valid, y_train, y_valid):\n",
    "    linear_regressor = LinearRegression(fit_intercept=True, copy_X=True, n_jobs=-1)\n",
    "    linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = linear_regressor.predict(X_valid)\n",
    "\n",
    "    metrics(y_valid=y_valid, y_pred=y_pred)\n",
    "\n",
    "\n",
    "Linear_Regression(X_train, X_valid, y_train, y_valid)\n",
    "Linear_Regression(X_train_norm, X_valid_norm, y_train, y_valid)\n",
    "Linear_Regression(X_train_min_max, X_valid_min_max, y_train, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial\n",
      "MSE: 0.00022930256444625305\n",
      "MAE: 0.010305776273156791\n",
      "R2 score: 0.963022201068232\n",
      "Normired\n",
      "MSE: 0.00022869910601888861\n",
      "MAE: 0.010301936122499999\n",
      "R2 score: 0.9631195160042627\n",
      "MinMaxScaler\n",
      "MSE: 0.00022609746896562008\n",
      "MAE: 0.010534004631613761\n",
      "R2 score: 0.9635390612984094\n"
     ]
    }
   ],
   "source": [
    "#Tree\n",
    "def Tree(X_train, X_valid, y_train, y_valid):\n",
    "    parameters = {'max_depth': range(3, 15, 1), 'min_samples_split': range(2, 9, 2), 'min_samples_leaf': range(1, 8, 2)}\n",
    "    tree = DecisionTreeRegressor()\n",
    "    tree_grid = GridSearchCV(tree, param_grid=parameters, cv=3, n_jobs=-1)\n",
    "    tree_grid.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = tree_grid.best_estimator_.predict(X_valid)\n",
    "\n",
    "    metrics(y_valid=y_valid, y_pred=y_pred)\n",
    "\n",
    "print('Initial')\n",
    "Tree(X_train, X_valid, y_train, y_valid)\n",
    "print('Normired')\n",
    "Tree(X_train_norm, X_valid_norm, y_train, y_valid)\n",
    "print('MinMaxScaler')\n",
    "Tree(X_train_min_max, X_valid_min_max, y_train, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial\n",
      "MSE: 0.00018313524050947173\n",
      "MAE: 0.009692133458152957\n",
      "R2 score: 0.9704672378294856\n",
      "Normired\n",
      "MSE: 0.00017662609649488986\n",
      "MAE: 0.00959571864208091\n",
      "R2 score: 0.971516915660915\n",
      "MinMaxScaler\n",
      "MSE: 0.000180513024752151\n",
      "MAE: 0.00964036281366034\n",
      "R2 score: 0.9708901016873935\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "def RandomForest(X_train, X_valid, y_train, y_valid):\n",
    "    parameters = {'max_depth': range(2, 15, 2), 'n_estimators': range(10, 100, 10), 'min_samples_split': range(2, 9, 2),\n",
    "                  'min_samples_leaf': range(1, 8, 2)}\n",
    "    forest = RandomForestRegressor()\n",
    "    forest_grid = RandomizedSearchCV(forest, param_distributions=parameters, cv=3, n_jobs=-1)\n",
    "    forest_grid.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = forest_grid.best_estimator_.predict(X_valid)\n",
    "\n",
    "    metrics(y_valid=y_valid, y_pred=y_pred)\n",
    "\n",
    "\n",
    "print('Initial')\n",
    "RandomForest(X_train, X_valid, y_train, y_valid)\n",
    "print('Normired')\n",
    "RandomForest(X_train_norm, X_valid_norm, y_train, y_valid)\n",
    "print('MinMaxScaler')\n",
    "RandomForest(X_train_min_max, X_valid_min_max, y_train, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial\n",
      "MSE: 0.0001460088524250124\n",
      "MAE: 0.008746320301586124\n",
      "R2 score: 0.9764543148469854\n",
      "Normired\n",
      "MSE: 0.0001515734888792323\n",
      "MAE: 0.008870382044893807\n",
      "R2 score: 0.975556950229937\n",
      "MinMaxScaler\n",
      "MSE: 0.0001527941663386823\n",
      "MAE: 0.008887858645514807\n",
      "R2 score: 0.97536010129471\n"
     ]
    }
   ],
   "source": [
    "#GradientBoosting\n",
    "def Gradient_Boost(X_train, X_valid, y_train, y_valid):\n",
    "    parameters = {'n_estimators': range(80, 140, 10),\n",
    "                  'max_depth': range(4, 14, 2),\n",
    "                  'criterion': ['friedman_mse', 'squared_error'],\n",
    "                  'learning_rate': np.arange(0.09, 0.12, 0.005),\n",
    "                  'min_samples_leaf': range(2, 14, 2),\n",
    "                  'min_samples_split': range(2, 14, 2),\n",
    "                  'random_state': [0]}\n",
    "    grad_boost = GradientBoostingRegressor()\n",
    "    grad_boost_grid = RandomizedSearchCV(grad_boost, param_distributions=parameters, cv=3, n_jobs=-1)\n",
    "    grad_boost_grid.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = grad_boost_grid.best_estimator_.predict(X_valid)\n",
    "\n",
    "    metrics(y_valid=y_valid, y_pred=y_pred)\n",
    "\n",
    "print('Initial')\n",
    "Gradient_Boost(X_train, X_valid, y_train, y_valid)\n",
    "print('Normired')\n",
    "Gradient_Boost(X_train_norm, X_valid_norm, y_train, y_valid)\n",
    "print('MinMaxScaler')\n",
    "Gradient_Boost(X_train_min_max, X_valid_min_max, y_train, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(21, n_hidden_neurons)\n",
    "        #self.act1 = nn.Sigmoid()\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons, n_hidden_neurons // 2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(n_hidden_neurons // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleNet(300)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape((8690, 1))\n",
    "X_valid_tensor = torch.tensor(X_valid_norm, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.float32).reshape((2173, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Training Loss: 42.1746, Valid Loss: 42.2241\n",
      "Epoch 11/10000, Training Loss: 2.1602, Valid Loss: 2.1584\n",
      "Epoch 21/10000, Training Loss: 1.4798, Valid Loss: 1.4794\n",
      "Epoch 31/10000, Training Loss: 0.4600, Valid Loss: 0.4614\n",
      "Epoch 41/10000, Training Loss: 0.1548, Valid Loss: 0.1416\n",
      "Epoch 51/10000, Training Loss: 0.1123, Valid Loss: 0.1018\n",
      "Epoch 61/10000, Training Loss: 0.0892, Valid Loss: 0.0821\n",
      "Epoch 71/10000, Training Loss: 0.0731, Valid Loss: 0.0685\n",
      "Epoch 81/10000, Training Loss: 0.0625, Valid Loss: 0.0601\n",
      "Epoch 91/10000, Training Loss: 0.0537, Valid Loss: 0.0518\n",
      "Epoch 101/10000, Training Loss: 0.0461, Valid Loss: 0.0448\n",
      "Epoch 111/10000, Training Loss: 0.0407, Valid Loss: 0.0397\n",
      "Epoch 121/10000, Training Loss: 0.0361, Valid Loss: 0.0355\n",
      "Epoch 131/10000, Training Loss: 0.0322, Valid Loss: 0.0320\n",
      "Epoch 141/10000, Training Loss: 0.0289, Valid Loss: 0.0289\n",
      "Epoch 151/10000, Training Loss: 0.0260, Valid Loss: 0.0262\n",
      "Epoch 161/10000, Training Loss: 0.0236, Valid Loss: 0.0239\n",
      "Epoch 171/10000, Training Loss: 0.0215, Valid Loss: 0.0219\n",
      "Epoch 181/10000, Training Loss: 0.0196, Valid Loss: 0.0202\n",
      "Epoch 191/10000, Training Loss: 0.0180, Valid Loss: 0.0186\n",
      "Epoch 201/10000, Training Loss: 0.0165, Valid Loss: 0.0171\n",
      "Epoch 211/10000, Training Loss: 0.0152, Valid Loss: 0.0158\n",
      "Epoch 221/10000, Training Loss: 0.0139, Valid Loss: 0.0147\n",
      "Epoch 231/10000, Training Loss: 0.0128, Valid Loss: 0.0136\n",
      "Epoch 241/10000, Training Loss: 0.0118, Valid Loss: 0.0125\n",
      "Epoch 251/10000, Training Loss: 0.0108, Valid Loss: 0.0116\n",
      "Epoch 261/10000, Training Loss: 0.0100, Valid Loss: 0.0108\n",
      "Epoch 271/10000, Training Loss: 0.0093, Valid Loss: 0.0100\n",
      "Epoch 281/10000, Training Loss: 0.0086, Valid Loss: 0.0093\n",
      "Epoch 291/10000, Training Loss: 0.0080, Valid Loss: 0.0087\n",
      "Epoch 301/10000, Training Loss: 0.0074, Valid Loss: 0.0082\n",
      "Epoch 311/10000, Training Loss: 0.0069, Valid Loss: 0.0076\n",
      "Epoch 321/10000, Training Loss: 0.0065, Valid Loss: 0.0072\n",
      "Epoch 331/10000, Training Loss: 0.0060, Valid Loss: 0.0067\n",
      "Epoch 341/10000, Training Loss: 0.0056, Valid Loss: 0.0063\n",
      "Epoch 351/10000, Training Loss: 0.0053, Valid Loss: 0.0059\n",
      "Epoch 361/10000, Training Loss: 0.0050, Valid Loss: 0.0056\n",
      "Epoch 371/10000, Training Loss: 0.0047, Valid Loss: 0.0053\n",
      "Epoch 381/10000, Training Loss: 0.0044, Valid Loss: 0.0050\n",
      "Epoch 391/10000, Training Loss: 0.0042, Valid Loss: 0.0048\n",
      "Epoch 401/10000, Training Loss: 0.0040, Valid Loss: 0.0046\n",
      "Epoch 411/10000, Training Loss: 0.0038, Valid Loss: 0.0044\n",
      "Epoch 421/10000, Training Loss: 0.0036, Valid Loss: 0.0042\n",
      "Epoch 431/10000, Training Loss: 0.0034, Valid Loss: 0.0040\n",
      "Epoch 441/10000, Training Loss: 0.0032, Valid Loss: 0.0039\n",
      "Epoch 451/10000, Training Loss: 0.0031, Valid Loss: 0.0037\n",
      "Epoch 461/10000, Training Loss: 0.0029, Valid Loss: 0.0036\n",
      "Epoch 471/10000, Training Loss: 0.0028, Valid Loss: 0.0034\n",
      "Epoch 481/10000, Training Loss: 0.0027, Valid Loss: 0.0033\n",
      "Epoch 491/10000, Training Loss: 0.0026, Valid Loss: 0.0032\n",
      "Epoch 501/10000, Training Loss: 0.0025, Valid Loss: 0.0031\n",
      "Epoch 511/10000, Training Loss: 0.0024, Valid Loss: 0.0030\n",
      "Epoch 521/10000, Training Loss: 0.0023, Valid Loss: 0.0029\n",
      "Epoch 531/10000, Training Loss: 0.0022, Valid Loss: 0.0028\n",
      "Epoch 541/10000, Training Loss: 0.0021, Valid Loss: 0.0027\n",
      "Epoch 551/10000, Training Loss: 0.0020, Valid Loss: 0.0026\n",
      "Epoch 561/10000, Training Loss: 0.0020, Valid Loss: 0.0025\n",
      "Epoch 571/10000, Training Loss: 0.0019, Valid Loss: 0.0025\n",
      "Epoch 581/10000, Training Loss: 0.0018, Valid Loss: 0.0024\n",
      "Epoch 591/10000, Training Loss: 0.0018, Valid Loss: 0.0023\n",
      "Epoch 601/10000, Training Loss: 0.0017, Valid Loss: 0.0023\n",
      "Epoch 611/10000, Training Loss: 0.0017, Valid Loss: 0.0022\n",
      "Epoch 621/10000, Training Loss: 0.0016, Valid Loss: 0.0021\n",
      "Epoch 631/10000, Training Loss: 0.0016, Valid Loss: 0.0021\n",
      "Epoch 641/10000, Training Loss: 0.0015, Valid Loss: 0.0020\n",
      "Epoch 651/10000, Training Loss: 0.0015, Valid Loss: 0.0020\n",
      "Epoch 661/10000, Training Loss: 0.0014, Valid Loss: 0.0020\n",
      "Epoch 671/10000, Training Loss: 0.0014, Valid Loss: 0.0019\n",
      "Epoch 681/10000, Training Loss: 0.0014, Valid Loss: 0.0019\n",
      "Epoch 691/10000, Training Loss: 0.0013, Valid Loss: 0.0018\n",
      "Epoch 701/10000, Training Loss: 0.0013, Valid Loss: 0.0018\n",
      "Epoch 711/10000, Training Loss: 0.0013, Valid Loss: 0.0018\n",
      "Epoch 721/10000, Training Loss: 0.0013, Valid Loss: 0.0017\n",
      "Epoch 731/10000, Training Loss: 0.0012, Valid Loss: 0.0017\n",
      "Epoch 741/10000, Training Loss: 0.0012, Valid Loss: 0.0017\n",
      "Epoch 751/10000, Training Loss: 0.0012, Valid Loss: 0.0016\n",
      "Epoch 761/10000, Training Loss: 0.0012, Valid Loss: 0.0016\n",
      "Epoch 771/10000, Training Loss: 0.0011, Valid Loss: 0.0016\n",
      "Epoch 781/10000, Training Loss: 0.0011, Valid Loss: 0.0016\n",
      "Epoch 791/10000, Training Loss: 0.0011, Valid Loss: 0.0015\n",
      "Epoch 801/10000, Training Loss: 0.0011, Valid Loss: 0.0015\n",
      "Epoch 811/10000, Training Loss: 0.0011, Valid Loss: 0.0015\n",
      "Epoch 821/10000, Training Loss: 0.0010, Valid Loss: 0.0015\n",
      "Epoch 831/10000, Training Loss: 0.0010, Valid Loss: 0.0015\n",
      "Epoch 841/10000, Training Loss: 0.0010, Valid Loss: 0.0014\n",
      "Epoch 851/10000, Training Loss: 0.0010, Valid Loss: 0.0014\n",
      "Epoch 861/10000, Training Loss: 0.0010, Valid Loss: 0.0014\n",
      "Epoch 871/10000, Training Loss: 0.0010, Valid Loss: 0.0014\n",
      "Epoch 881/10000, Training Loss: 0.0009, Valid Loss: 0.0014\n",
      "Epoch 891/10000, Training Loss: 0.0009, Valid Loss: 0.0013\n",
      "Epoch 901/10000, Training Loss: 0.0009, Valid Loss: 0.0013\n",
      "Epoch 911/10000, Training Loss: 0.0009, Valid Loss: 0.0013\n",
      "Epoch 921/10000, Training Loss: 0.0009, Valid Loss: 0.0013\n",
      "Epoch 931/10000, Training Loss: 0.0009, Valid Loss: 0.0013\n",
      "Epoch 941/10000, Training Loss: 0.0009, Valid Loss: 0.0013\n",
      "Epoch 951/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 961/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 971/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 981/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 991/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 1001/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 1011/10000, Training Loss: 0.0008, Valid Loss: 0.0012\n",
      "Epoch 1021/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 1031/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 1041/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 1051/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1061/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1071/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1081/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1091/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1101/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1111/10000, Training Loss: 0.0007, Valid Loss: 0.0011\n",
      "Epoch 1121/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1131/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1141/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1151/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1161/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1171/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1181/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1191/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1201/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1211/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1221/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1231/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1241/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1251/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1261/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1271/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1281/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1291/10000, Training Loss: 0.0006, Valid Loss: 0.0010\n",
      "Epoch 1301/10000, Training Loss: 0.0038, Valid Loss: 0.0042\n",
      "Epoch 1311/10000, Training Loss: 0.0010, Valid Loss: 0.0012\n",
      "Epoch 1321/10000, Training Loss: 0.0020, Valid Loss: 0.0024\n",
      "Epoch 1331/10000, Training Loss: 0.0012, Valid Loss: 0.0015\n",
      "Epoch 1341/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1351/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1361/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1371/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1381/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1391/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1401/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1411/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1421/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1431/10000, Training Loss: 0.0051, Valid Loss: 0.0054\n",
      "Epoch 1441/10000, Training Loss: 0.0008, Valid Loss: 0.0010\n",
      "Epoch 1451/10000, Training Loss: 0.0021, Valid Loss: 0.0024\n",
      "Epoch 1461/10000, Training Loss: 0.0016, Valid Loss: 0.0019\n",
      "Epoch 1471/10000, Training Loss: 0.0011, Valid Loss: 0.0014\n",
      "Epoch 1481/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1491/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1501/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1511/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1521/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1531/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1541/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1551/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1561/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1571/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1581/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1591/10000, Training Loss: 0.0008, Valid Loss: 0.0010\n",
      "Epoch 1601/10000, Training Loss: 0.0491, Valid Loss: 0.0480\n",
      "Epoch 1611/10000, Training Loss: 0.0156, Valid Loss: 0.0158\n",
      "Epoch 1621/10000, Training Loss: 0.0052, Valid Loss: 0.0055\n",
      "Epoch 1631/10000, Training Loss: 0.0022, Valid Loss: 0.0025\n",
      "Epoch 1641/10000, Training Loss: 0.0011, Valid Loss: 0.0014\n",
      "Epoch 1651/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 1661/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1671/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1681/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 1691/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 1701/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1711/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1721/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1731/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1741/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1751/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1761/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1771/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1781/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1791/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 1801/10000, Training Loss: 0.0167, Valid Loss: 0.0172\n",
      "Epoch 1811/10000, Training Loss: 0.0140, Valid Loss: 0.0137\n",
      "Epoch 1821/10000, Training Loss: 0.0047, Valid Loss: 0.0047\n",
      "Epoch 1831/10000, Training Loss: 0.0063, Valid Loss: 0.0065\n",
      "Epoch 1841/10000, Training Loss: 0.0030, Valid Loss: 0.0031\n",
      "Epoch 1851/10000, Training Loss: 0.0013, Valid Loss: 0.0015\n",
      "Epoch 1861/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 1871/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 1881/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 1891/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1901/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 1911/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 1921/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 1931/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 1941/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1951/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1961/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1971/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1981/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 1991/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 2001/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2011/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2021/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2031/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2041/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2051/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 2061/10000, Training Loss: 0.0321, Valid Loss: 0.0319\n",
      "Epoch 2071/10000, Training Loss: 0.0821, Valid Loss: 0.0822\n",
      "Epoch 2081/10000, Training Loss: 0.0232, Valid Loss: 0.0235\n",
      "Epoch 2091/10000, Training Loss: 0.0089, Valid Loss: 0.0089\n",
      "Epoch 2101/10000, Training Loss: 0.0042, Valid Loss: 0.0045\n",
      "Epoch 2111/10000, Training Loss: 0.0012, Valid Loss: 0.0014\n",
      "Epoch 2121/10000, Training Loss: 0.0009, Valid Loss: 0.0012\n",
      "Epoch 2131/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 2141/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2151/10000, Training Loss: 0.0006, Valid Loss: 0.0009\n",
      "Epoch 2161/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2171/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2181/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2191/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2201/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 2211/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 2221/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2231/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2241/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2251/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2261/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2271/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2281/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2291/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2301/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2311/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2321/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2331/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2341/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2351/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2361/10000, Training Loss: 0.0004, Valid Loss: 0.0007\n",
      "Epoch 2371/10000, Training Loss: 0.0004, Valid Loss: 0.0007\n",
      "Epoch 2381/10000, Training Loss: 0.0007, Valid Loss: 0.0008\n",
      "Epoch 2391/10000, Training Loss: 0.0571, Valid Loss: 0.0570\n",
      "Epoch 2401/10000, Training Loss: 0.0742, Valid Loss: 0.0747\n",
      "Epoch 2411/10000, Training Loss: 0.0148, Valid Loss: 0.0151\n",
      "Epoch 2421/10000, Training Loss: 0.0097, Valid Loss: 0.0099\n",
      "Epoch 2431/10000, Training Loss: 0.0026, Valid Loss: 0.0028\n",
      "Epoch 2441/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 2451/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 2461/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2471/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2481/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2491/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2501/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2511/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2521/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2531/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2541/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2551/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2561/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2571/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2581/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2591/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2601/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2611/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2621/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2631/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2641/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2651/10000, Training Loss: 0.0114, Valid Loss: 0.0118\n",
      "Epoch 2661/10000, Training Loss: 0.0176, Valid Loss: 0.0177\n",
      "Epoch 2671/10000, Training Loss: 0.0218, Valid Loss: 0.0220\n",
      "Epoch 2681/10000, Training Loss: 0.0037, Valid Loss: 0.0039\n",
      "Epoch 2691/10000, Training Loss: 0.0018, Valid Loss: 0.0020\n",
      "Epoch 2701/10000, Training Loss: 0.0011, Valid Loss: 0.0013\n",
      "Epoch 2711/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2721/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2731/10000, Training Loss: 0.0006, Valid Loss: 0.0007\n",
      "Epoch 2741/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2751/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2761/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2771/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2781/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2791/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2801/10000, Training Loss: 0.0004, Valid Loss: 0.0007\n",
      "Epoch 2811/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2821/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2831/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2841/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2851/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2861/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2871/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 2881/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2891/10000, Training Loss: 0.0645, Valid Loss: 0.0641\n",
      "Epoch 2901/10000, Training Loss: 0.0686, Valid Loss: 0.0689\n",
      "Epoch 2911/10000, Training Loss: 0.0224, Valid Loss: 0.0228\n",
      "Epoch 2921/10000, Training Loss: 0.0050, Valid Loss: 0.0053\n",
      "Epoch 2931/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2941/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2951/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 2961/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 2971/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2981/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 2991/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3001/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3011/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3021/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3031/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3041/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3051/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3061/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3071/10000, Training Loss: 0.0017, Valid Loss: 0.0019\n",
      "Epoch 3081/10000, Training Loss: 0.0035, Valid Loss: 0.0036\n",
      "Epoch 3091/10000, Training Loss: 0.0015, Valid Loss: 0.0017\n",
      "Epoch 3101/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3111/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 3121/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 3131/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3141/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3151/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3161/10000, Training Loss: 0.0030, Valid Loss: 0.0031\n",
      "Epoch 3171/10000, Training Loss: 0.0921, Valid Loss: 0.0913\n",
      "Epoch 3181/10000, Training Loss: 0.0198, Valid Loss: 0.0202\n",
      "Epoch 3191/10000, Training Loss: 0.0056, Valid Loss: 0.0059\n",
      "Epoch 3201/10000, Training Loss: 0.0008, Valid Loss: 0.0010\n",
      "Epoch 3211/10000, Training Loss: 0.0013, Valid Loss: 0.0015\n",
      "Epoch 3221/10000, Training Loss: 0.0006, Valid Loss: 0.0007\n",
      "Epoch 3231/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 3241/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3251/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3261/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3271/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3281/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3291/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3301/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3311/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3321/10000, Training Loss: 0.0062, Valid Loss: 0.0064\n",
      "Epoch 3331/10000, Training Loss: 0.0041, Valid Loss: 0.0042\n",
      "Epoch 3341/10000, Training Loss: 0.0037, Valid Loss: 0.0040\n",
      "Epoch 3351/10000, Training Loss: 0.0563, Valid Loss: 0.0569\n",
      "Epoch 3361/10000, Training Loss: 0.0210, Valid Loss: 0.0209\n",
      "Epoch 3371/10000, Training Loss: 0.0014, Valid Loss: 0.0016\n",
      "Epoch 3381/10000, Training Loss: 0.0020, Valid Loss: 0.0023\n",
      "Epoch 3391/10000, Training Loss: 0.0010, Valid Loss: 0.0012\n",
      "Epoch 3401/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3411/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3421/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3431/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3441/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3451/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3461/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3471/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3481/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3491/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 3501/10000, Training Loss: 0.0232, Valid Loss: 0.0237\n",
      "Epoch 3511/10000, Training Loss: 0.0035, Valid Loss: 0.0035\n",
      "Epoch 3521/10000, Training Loss: 0.0134, Valid Loss: 0.0139\n",
      "Epoch 3531/10000, Training Loss: 0.0060, Valid Loss: 0.0063\n",
      "Epoch 3541/10000, Training Loss: 0.0012, Valid Loss: 0.0014\n",
      "Epoch 3551/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3561/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 3571/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3581/10000, Training Loss: 0.0014, Valid Loss: 0.0017\n",
      "Epoch 3591/10000, Training Loss: 0.0036, Valid Loss: 0.0038\n",
      "Epoch 3601/10000, Training Loss: 0.0013, Valid Loss: 0.0015\n",
      "Epoch 3611/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 3621/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3631/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3641/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3651/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3661/10000, Training Loss: 0.0008, Valid Loss: 0.0010\n",
      "Epoch 3671/10000, Training Loss: 0.0075, Valid Loss: 0.0077\n",
      "Epoch 3681/10000, Training Loss: 0.0048, Valid Loss: 0.0048\n",
      "Epoch 3691/10000, Training Loss: 0.0248, Valid Loss: 0.0247\n",
      "Epoch 3701/10000, Training Loss: 0.0011, Valid Loss: 0.0013\n",
      "Epoch 3711/10000, Training Loss: 0.0015, Valid Loss: 0.0016\n",
      "Epoch 3721/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3731/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3741/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3751/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3761/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3771/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3781/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3791/10000, Training Loss: 0.0030, Valid Loss: 0.0031\n",
      "Epoch 3801/10000, Training Loss: 0.0728, Valid Loss: 0.0724\n",
      "Epoch 3811/10000, Training Loss: 0.0190, Valid Loss: 0.0195\n",
      "Epoch 3821/10000, Training Loss: 0.0041, Valid Loss: 0.0044\n",
      "Epoch 3831/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3841/10000, Training Loss: 0.0013, Valid Loss: 0.0015\n",
      "Epoch 3851/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3861/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 3871/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3881/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3891/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3901/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3911/10000, Training Loss: 0.0041, Valid Loss: 0.0043\n",
      "Epoch 3921/10000, Training Loss: 0.0153, Valid Loss: 0.0157\n",
      "Epoch 3931/10000, Training Loss: 0.0056, Valid Loss: 0.0056\n",
      "Epoch 3941/10000, Training Loss: 0.0017, Valid Loss: 0.0020\n",
      "Epoch 3951/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 3961/10000, Training Loss: 0.0006, Valid Loss: 0.0007\n",
      "Epoch 3971/10000, Training Loss: 0.0004, Valid Loss: 0.0007\n",
      "Epoch 3981/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 3991/10000, Training Loss: 0.0016, Valid Loss: 0.0019\n",
      "Epoch 4001/10000, Training Loss: 0.0187, Valid Loss: 0.0191\n",
      "Epoch 4011/10000, Training Loss: 0.0012, Valid Loss: 0.0015\n",
      "Epoch 4021/10000, Training Loss: 0.0008, Valid Loss: 0.0011\n",
      "Epoch 4031/10000, Training Loss: 0.0017, Valid Loss: 0.0018\n",
      "Epoch 4041/10000, Training Loss: 0.0011, Valid Loss: 0.0014\n",
      "Epoch 4051/10000, Training Loss: 0.0007, Valid Loss: 0.0008\n",
      "Epoch 4061/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 4071/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4081/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4091/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4101/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4111/10000, Training Loss: 0.0015, Valid Loss: 0.0017\n",
      "Epoch 4121/10000, Training Loss: 0.0069, Valid Loss: 0.0069\n",
      "Epoch 4131/10000, Training Loss: 0.0028, Valid Loss: 0.0029\n",
      "Epoch 4141/10000, Training Loss: 0.1273, Valid Loss: 0.1264\n",
      "Epoch 4151/10000, Training Loss: 0.0037, Valid Loss: 0.0037\n",
      "Epoch 4161/10000, Training Loss: 0.0016, Valid Loss: 0.0018\n",
      "Epoch 4171/10000, Training Loss: 0.0015, Valid Loss: 0.0016\n",
      "Epoch 4181/10000, Training Loss: 0.0010, Valid Loss: 0.0011\n",
      "Epoch 4191/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 4201/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4211/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4221/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4231/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4241/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4251/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4261/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4271/10000, Training Loss: 0.0027, Valid Loss: 0.0029\n",
      "Epoch 4281/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 4291/10000, Training Loss: 0.0019, Valid Loss: 0.0020\n",
      "Epoch 4301/10000, Training Loss: 0.0008, Valid Loss: 0.0010\n",
      "Epoch 4311/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4321/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4331/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4341/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4351/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4361/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4371/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4381/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 4391/10000, Training Loss: 0.0212, Valid Loss: 0.0216\n",
      "Epoch 4401/10000, Training Loss: 0.0053, Valid Loss: 0.0053\n",
      "Epoch 4411/10000, Training Loss: 0.0053, Valid Loss: 0.0056\n",
      "Epoch 4421/10000, Training Loss: 0.0043, Valid Loss: 0.0046\n",
      "Epoch 4431/10000, Training Loss: 0.0019, Valid Loss: 0.0022\n",
      "Epoch 4441/10000, Training Loss: 0.0011, Valid Loss: 0.0013\n",
      "Epoch 4451/10000, Training Loss: 0.0053, Valid Loss: 0.0053\n",
      "Epoch 4461/10000, Training Loss: 0.0018, Valid Loss: 0.0020\n",
      "Epoch 4471/10000, Training Loss: 0.0007, Valid Loss: 0.0008\n",
      "Epoch 4481/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4491/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4501/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4511/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4521/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4531/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4541/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4551/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4561/10000, Training Loss: 0.0077, Valid Loss: 0.0077\n",
      "Epoch 4571/10000, Training Loss: 0.0123, Valid Loss: 0.0125\n",
      "Epoch 4581/10000, Training Loss: 0.0028, Valid Loss: 0.0030\n",
      "Epoch 4591/10000, Training Loss: 0.0017, Valid Loss: 0.0019\n",
      "Epoch 4601/10000, Training Loss: 0.0035, Valid Loss: 0.0037\n",
      "Epoch 4611/10000, Training Loss: 0.0123, Valid Loss: 0.0127\n",
      "Epoch 4621/10000, Training Loss: 0.0005, Valid Loss: 0.0008\n",
      "Epoch 4631/10000, Training Loss: 0.0019, Valid Loss: 0.0020\n",
      "Epoch 4641/10000, Training Loss: 0.0010, Valid Loss: 0.0012\n",
      "Epoch 4651/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4661/10000, Training Loss: 0.0004, Valid Loss: 0.0005\n",
      "Epoch 4671/10000, Training Loss: 0.0007, Valid Loss: 0.0009\n",
      "Epoch 4681/10000, Training Loss: 0.0067, Valid Loss: 0.0067\n",
      "Epoch 4691/10000, Training Loss: 0.0270, Valid Loss: 0.0268\n",
      "Epoch 4701/10000, Training Loss: 0.0054, Valid Loss: 0.0058\n",
      "Epoch 4711/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 4721/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 4731/10000, Training Loss: 0.0006, Valid Loss: 0.0008\n",
      "Epoch 4741/10000, Training Loss: 0.0004, Valid Loss: 0.0006\n",
      "Epoch 4751/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4761/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4771/10000, Training Loss: 0.0003, Valid Loss: 0.0005\n",
      "Epoch 4781/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n",
      "Epoch 4791/10000, Training Loss: 0.0067, Valid Loss: 0.0067\n",
      "Epoch 4801/10000, Training Loss: 0.0048, Valid Loss: 0.0050\n",
      "Epoch 4811/10000, Training Loss: 0.0320, Valid Loss: 0.0318\n",
      "Epoch 4821/10000, Training Loss: 0.0081, Valid Loss: 0.0085\n",
      "Epoch 4831/10000, Training Loss: 0.0052, Valid Loss: 0.0053\n",
      "Epoch 4841/10000, Training Loss: 0.0007, Valid Loss: 0.0010\n",
      "Epoch 4851/10000, Training Loss: 0.0004, Valid Loss: 0.0007\n",
      "Epoch 4861/10000, Training Loss: 0.0005, Valid Loss: 0.0007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[75], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m y_valid_pred_tensor \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(X_valid_tensor)\n\u001B[0;32m     10\u001B[0m valid_loss_val \u001B[38;5;241m=\u001B[39m criterion(y_valid_pred_tensor, y_valid_tensor)\n\u001B[1;32m---> 12\u001B[0m \u001B[43mtrain_loss_val\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mF:\\python\\Lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\python\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred_tensor = model.forward(X_train_tensor)\n",
    "    train_loss_val = criterion(y_pred_tensor, y_train_tensor)\n",
    "\n",
    "    y_valid_pred_tensor = model.forward(X_valid_tensor)\n",
    "    valid_loss_val = criterion(y_valid_pred_tensor, y_valid_tensor)\n",
    "\n",
    "    train_loss_val.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss_val:.4f}, Valid Loss: {valid_loss_val:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mae(pred, target):\n",
    "    abs = torch.abs(pred - target)\n",
    "    return abs.mean()\n",
    "\n",
    "\n",
    "def r_squared(y_pred, y_true):\n",
    "    return r2_score(y_true.detach().numpy(), y_pred.detach().numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(criterion(model.forward(X_valid_tensor), y_valid_tensor))\n",
    "print(mae(model.forward(X_valid_tensor), y_valid_tensor))\n",
    "print(r_squared(model.forward(X_valid_tensor), y_valid_tensor))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
